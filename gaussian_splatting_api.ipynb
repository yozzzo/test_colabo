{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Gaussian Splatting API on Google Colab\n",
    "\n",
    "This notebook creates a FastAPI server that:\n",
    "- Accepts image/video uploads via HTTP POST\n",
    "- Extracts frames from videos (if needed)\n",
    "- Runs COLMAP for camera pose estimation\n",
    "- Trains a 3D Gaussian Splatting model\n",
    "- Returns the trained model (.ply file) via Google Drive\n",
    "\n",
    "**Note**: T4 GPU (16GB) has less VRAM than recommended (24GB), so we'll use reduced settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Install System Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Install system dependencies\n!apt-get update && apt-get install -y \\\n    libglew-dev \\\n    libassimp-dev \\\n    libboost-all-dev \\\n    libgtk-3-dev \\\n    libopencv-dev \\\n    libglfw3-dev \\\n    libavdevice-dev \\\n    libavcodec-dev \\\n    libeigen3-dev \\\n    libxxf86vm-dev \\\n    libembree-dev \\\n    cmake \\\n    imagemagick \\\n    ffmpeg\n\n# Fix ImageMagick policy for PDF/PS files (sometimes needed)\n!sed -i '/disable ghostscript format types/,+6d' /etc/ImageMagick-6/policy.xml || true"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install COLMAP (prebuilt binary for faster setup)\n",
    "!wget https://github.com/colmap/colmap/releases/download/3.8/colmap-3.8-linux-cuda.tar.gz\n",
    "!tar -xzf colmap-3.8-linux-cuda.tar.gz\n",
    "!cp -r colmap-3.8-linux-cuda/* /usr/local/\n",
    "!rm -rf colmap-3.8-linux-cuda*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Clone and Setup Gaussian Splatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/graphdeco-inria/gaussian-splatting --recursive\n",
    "%cd gaussian-splatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "!pip install -q plyfile tqdm opencv-python joblib\n",
    "!pip install -q fastapi uvicorn pyngrok nest-asyncio python-multipart aiofiles\n",
    "\n",
    "# Install submodules\n",
    "!pip install -q submodules/diff-gaussian-rasterization\n",
    "!pip install -q submodules/simple-knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Setup API Server"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport shutil\nimport asyncio\nimport nest_asyncio\nimport subprocess\nimport cv2\nfrom pathlib import Path\nfrom datetime import datetime\nfrom contextlib import asynccontextmanager\nfrom typing import List, Optional\n\nfrom fastapi import FastAPI, UploadFile, File, HTTPException, Header, Form\nfrom fastapi.responses import JSONResponse\nimport uvicorn\nfrom pyngrok import ngrok\nfrom google.colab import drive, userdata\n\n# Add gaussian-splatting to path\nsys.path.append('/content/gaussian-splatting')\n\n# Enable nested event loops\nnest_asyncio.apply()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\n# Get secrets from Colab userdata\ntry:\n    NGROK_AUTHTOKEN = userdata.get('NGROK_AUTHTOKEN')\n    API_KEY = userdata.get('API_KEY')\nexcept Exception as e:\n    print(\"âš ï¸  Warning: Could not load secrets from Colab userdata\")\n    print(\"Please set NGROK_AUTHTOKEN and API_KEY in Colab secrets\")\n    print(\"Settings â†’ Secrets â†’ Add new secret\")\n    NGROK_AUTHTOKEN = None\n    API_KEY = None\n\nUPLOAD_DIR = Path(\"/content/uploads\")\nDATASET_DIR = Path(\"/content/datasets\")\nOUTPUT_DIR = Path(\"/content/drive/MyDrive/gaussian_splatting_outputs\")\n\n# Create directories\nUPLOAD_DIR.mkdir(exist_ok=True)\nDATASET_DIR.mkdir(exist_ok=True)\n\n# Set ngrok auth token\nif NGROK_AUTHTOKEN:\n    ngrok.set_auth_token(NGROK_AUTHTOKEN)\nelse:\n    raise ValueError(\"NGROK_AUTHTOKEN not found in Colab secrets\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def extract_frames_from_video(video_path: Path, output_dir: Path, fps: int = 2) -> List[Path]:\n    \"\"\"Extract frames from video at specified FPS\"\"\"\n    output_dir.mkdir(exist_ok=True)\n    \n    cap = cv2.VideoCapture(str(video_path))\n    video_fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_interval = int(video_fps / fps)\n    \n    frame_paths = []\n    frame_count = 0\n    saved_count = 0\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n            \n        if frame_count % frame_interval == 0:\n            frame_path = output_dir / f\"frame_{saved_count:04d}.jpg\"\n            cv2.imwrite(str(frame_path), frame)\n            frame_paths.append(frame_path)\n            saved_count += 1\n            \n        frame_count += 1\n    \n    cap.release()\n    print(f\"Extracted {saved_count} frames from video\")\n    return frame_paths\n\n\ndef prepare_dataset_from_images(image_dir: Path, dataset_name: str) -> Path:\n    \"\"\"Prepare dataset structure for Gaussian Splatting\"\"\"\n    dataset_path = DATASET_DIR / dataset_name\n    dataset_path.mkdir(exist_ok=True)\n    \n    # Create input directory for COLMAP\n    input_dir = dataset_path / \"input\"\n    input_dir.mkdir(exist_ok=True)\n    \n    # Copy images to input directory\n    image_count = 0\n    for img_path in list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\")):\n        shutil.copy(img_path, input_dir)\n        image_count += 1\n    \n    print(f\"Copied {image_count} images to {input_dir}\")\n    return dataset_path\n\n\ndef validate_dataset_structure(dataset_path: Path) -> bool:\n    \"\"\"Validate that dataset has correct structure for training\"\"\"\n    print(f\"\\nValidating dataset structure at: {dataset_path}\")\n    \n    # List all directories in dataset\n    print(\"\\nDataset directories:\")\n    for p in sorted(dataset_path.iterdir()):\n        if p.is_dir():\n            print(f\"  - {p.name}/\")\n            # List subdirectories\n            for sp in sorted(p.iterdir())[:5]:\n                if sp.is_dir():\n                    print(f\"    - {sp.name}/\")\n    \n    # Check for images directory (created by convert.py)\n    images_dir = dataset_path / \"images\"\n    if not images_dir.exists():\n        print(f\"âŒ Missing required directory: images\")\n        # Check if images are in different location\n        possible_dirs = [\"images_2\", \"images_4\", \"images_8\"]\n        for dir_name in possible_dirs:\n            if (dataset_path / dir_name).exists():\n                print(f\"  Found: {dir_name}\")\n        return False\n    else:\n        img_count = len(list(images_dir.glob(\"*\")))\n        print(f\"âœ“ Found images directory with {img_count} files\")\n    \n    # Check for sparse reconstruction\n    sparse_dir = dataset_path / \"sparse\"\n    if not sparse_dir.exists():\n        print(f\"âŒ Missing required directory: sparse\")\n        return False\n    \n    # Find numbered sparse subdirectories\n    sparse_subdirs = [d for d in sparse_dir.iterdir() if d.is_dir() and d.name.isdigit()]\n    if not sparse_subdirs:\n        print(\"âŒ No numbered sparse directories found\")\n        # List what's actually in sparse\n        print(f\"  Sparse directory contents: {list(sparse_dir.iterdir())}\")\n        return False\n    \n    # Get the latest sparse directory (highest number)\n    sparse_subdir = max(sparse_subdirs, key=lambda x: int(x.name))\n    print(f\"âœ“ Using sparse directory: {sparse_subdir}\")\n    \n    # Check for required COLMAP output files\n    required_files = [\"cameras.bin\", \"images.bin\", \"points3D.bin\"]\n    missing_files = []\n    for file_name in required_files:\n        bin_path = sparse_subdir / file_name\n        txt_path = sparse_subdir / file_name.replace(\".bin\", \".txt\")\n        if not bin_path.exists() and not txt_path.exists():\n            missing_files.append(file_name)\n        else:\n            print(f\"  âœ“ Found: {file_name}\")\n    \n    if missing_files:\n        print(f\"âŒ Missing required files: {missing_files}\")\n        return False\n    \n    print(\"âœ“ Dataset structure validated successfully\")\n    return True\n\n\nasync def run_colmap(dataset_path: Path):\n    \"\"\"Run COLMAP for structure from motion using convert.py\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Running COLMAP reconstruction...\")\n    print(\"=\"*60)\n    \n    # Verify input images exist\n    input_dir = dataset_path / \"input\"\n    input_images = list(input_dir.glob(\"*.jpg\")) + list(input_dir.glob(\"*.png\"))\n    \n    if not input_images:\n        raise Exception(f\"No images found in {input_dir}\")\n    \n    print(f\"Found {len(input_images)} input images\")\n    \n    # Check image quality (basic check)\n    print(\"\\nChecking images:\")\n    for i, img_path in enumerate(input_images[:5]):  # Check first 5\n        img = cv2.imread(str(img_path))\n        if img is not None:\n            h, w = img.shape[:2]\n            print(f\"  - {img_path.name}: {w}x{h}\")\n        else:\n            print(f\"  - {img_path.name}: Failed to load!\")\n    \n    # Run convert.py which handles all COLMAP processing\n    cmd = [\n        \"python\", \"/content/gaussian-splatting/convert.py\",\n        \"-s\", str(dataset_path)\n        # Removed --resize to use full resolution by default\n    ]\n    \n    print(f\"\\nRunning command: {' '.join(cmd)}\")\n    process = subprocess.run(cmd, capture_output=True, text=True)\n    \n    # Always print output for debugging\n    print(\"\\n--- COLMAP Output ---\")\n    if process.stdout:\n        print(\"STDOUT:\")\n        print(process.stdout)\n    if process.stderr:\n        print(\"\\nSTDERR:\")\n        print(process.stderr)\n    print(\"--- End COLMAP Output ---\\n\")\n    \n    if process.returncode != 0:\n        # Try to identify specific error\n        if \"Could not register\" in process.stderr:\n            print(\"âš ï¸  COLMAP failed to find enough feature matches between images.\")\n            print(\"Tips: Ensure images have good overlap (>70%) and are not too blurry.\")\n        raise Exception(f\"COLMAP failed with return code {process.returncode}\")\n    \n    # Validate the output structure\n    if not validate_dataset_structure(dataset_path):\n        # Try alternative approach: run COLMAP manually\n        print(\"\\nâš ï¸  Standard convert.py failed. Trying manual COLMAP approach...\")\n        \n        # Create necessary directories\n        (dataset_path / \"distorted\").mkdir(exist_ok=True)\n        (dataset_path / \"images\").mkdir(exist_ok=True)\n        (dataset_path / \"sparse\").mkdir(exist_ok=True)\n        (dataset_path / \"stereo\").mkdir(exist_ok=True)\n        \n        # Copy images to images directory as fallback\n        for img in input_images:\n            shutil.copy(img, dataset_path / \"images\" / img.name)\n        \n        # Create minimal sparse reconstruction\n        sparse_0 = dataset_path / \"sparse\" / \"0\"\n        sparse_0.mkdir(exist_ok=True)\n        \n        # Check again\n        if not validate_dataset_structure(dataset_path):\n            raise Exception(\"COLMAP output structure is invalid\")\n    \n    print(\"\\nâœ“ COLMAP completed successfully\")\n\n\nasync def train_gaussian_splatting(dataset_path: Path, iterations: int = 7000) -> Path:\n    \"\"\"Train Gaussian Splatting model with T4-optimized settings\"\"\"\n    print(f\"\\nTraining Gaussian Splatting for {iterations} iterations...\")\n    \n    # Validate dataset before training\n    if not validate_dataset_structure(dataset_path):\n        raise Exception(\"Dataset structure is invalid for training\")\n    \n    output_path = dataset_path / \"output\"\n    \n    # Build command with correct parameters for T4 GPU\n    cmd = [\n        \"python\", \"/content/gaussian-splatting/train.py\",\n        \"-s\", str(dataset_path),\n        \"-m\", str(output_path),\n        \"--iterations\", str(iterations),\n        \"--densify_until_iter\", str(min(iterations, 5000)),  # Don't exceed iterations\n        \"--densification_interval\", \"100\",\n        \"--position_lr_max_steps\", str(iterations),\n        \"--save_iterations\", str(iterations),  # Save at final iteration\n        \"--quiet\"  # Reduce output verbosity\n    ]\n    \n    # Add T4 GPU optimizations for limited memory\n    if iterations <= 1000:  # For quick testing\n        cmd.extend([\n            \"--test_iterations\", str(iterations),\n            \"--checkpoint_iterations\", str(iterations),\n            \"--sh_degree\", \"2\"  # Reduce spherical harmonics for T4 GPU\n        ])\n    else:\n        cmd.extend([\n            \"--test_iterations\", \"1000\", \"3000\", \"5000\", \"7000\",\n            \"--checkpoint_iterations\", \"7000\"\n        ])\n    \n    print(f\"Running training command: {' '.join(cmd)}\")\n    process = subprocess.run(cmd, capture_output=True, text=True)\n    \n    if process.returncode != 0:\n        print(f\"Training stderr: {process.stderr}\")\n        raise Exception(f\"Training failed: {process.stderr}\")\n    \n    # Find the output PLY file\n    ply_path = output_path / \"point_cloud\" / f\"iteration_{iterations}\" / \"point_cloud.ply\"\n    \n    if not ply_path.exists():\n        # List actual output structure\n        point_cloud_dir = output_path / \"point_cloud\"\n        if point_cloud_dir.exists():\n            print(f\"Available iterations: {[d.name for d in point_cloud_dir.iterdir() if d.is_dir()]}\")\n        raise Exception(f\"Expected output not found at {ply_path}\")\n    \n    print(f\"âœ“ Training completed successfully. Output: {ply_path}\")\n    return ply_path"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ FastAPI Application"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    print(\"Starting 3D Gaussian Splatting API...\")\n    yield\n    print(\"Shutting down...\")\n\napp = FastAPI(\n    title=\"3D Gaussian Splatting API\",\n    description=\"Convert images/videos to 3D Gaussian Splatting models\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n\ndef verify_api_key(api_key: str = Header(None)):\n    if api_key != API_KEY:\n        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n    return api_key\n\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"status\": \"online\",\n        \"message\": \"3D Gaussian Splatting API is running\",\n        \"endpoints\": [\"/\", \"/process\"],\n        \"gpu\": \"T4 (16GB VRAM)\"\n    }\n\n\n@app.post(\"/process\")\nasync def process_gaussian_splatting(\n    files: List[UploadFile] = File(...),\n    iterations: Optional[int] = Form(1000),  # Reduced default for testing\n    extract_fps: Optional[int] = Form(2),\n    api_key: str = Header(None)\n):\n    \"\"\"Process images/video to create 3D Gaussian Splatting model\"\"\"\n    \n    verify_api_key(api_key)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    job_name = f\"gs_{timestamp}\"\n    job_dir = UPLOAD_DIR / job_name\n    job_dir.mkdir(exist_ok=True)\n    \n    try:\n        # Process uploaded files\n        image_dir = job_dir / \"images\"\n        image_dir.mkdir(exist_ok=True)\n        \n        for file in files:\n            file_path = job_dir / file.filename\n            \n            # Save uploaded file\n            with open(file_path, \"wb\") as f:\n                content = await file.read()\n                f.write(content)\n            \n            # Check if video or image\n            if file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n                # Extract frames from video\n                extract_frames_from_video(file_path, image_dir, fps=extract_fps)\n            else:\n                # Copy image to image directory\n                shutil.copy(file_path, image_dir)\n        \n        # Check if we have enough images\n        image_count = len(list(image_dir.glob(\"*\")))\n        if image_count < 3:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Need at least 3 images, got {image_count}\"\n            )\n        \n        # For optimal results, warn if too few images\n        if image_count < 10:\n            print(f\"Warning: Only {image_count} images. For better results, use 10+ images with good overlap.\")\n        \n        print(f\"Processing {image_count} images...\")\n        \n        # Prepare dataset structure\n        dataset_path = prepare_dataset_from_images(image_dir, job_name)\n        \n        # Run COLMAP reconstruction\n        await run_colmap(dataset_path)\n        \n        # Train Gaussian Splatting model\n        ply_path = await train_gaussian_splatting(dataset_path, iterations)\n        \n        # Copy results to Google Drive\n        output_filename = f\"{job_name}_gaussian_splatting.ply\"\n        drive_path = OUTPUT_DIR / output_filename\n        shutil.copy(ply_path, drive_path)\n        \n        # Also save the entire output directory as zip (optional, might be large)\n        output_zip = f\"{job_name}_full_output\"\n        shutil.make_archive(\n            str(OUTPUT_DIR / output_zip),\n            'zip',\n            dataset_path / \"output\"\n        )\n        \n        return JSONResponse({\n            \"status\": \"success\",\n            \"job_id\": job_name,\n            \"model_file\": output_filename,\n            \"download_path\": f\"/content/drive/MyDrive/gaussian_splatting_outputs/{output_filename}\",\n            \"full_output_zip\": f\"{output_zip}.zip\",\n            \"images_processed\": image_count,\n            \"iterations\": iterations,\n            \"completed_at\": datetime.now().isoformat()\n        })\n        \n    except Exception as e:\n        # Log the full error for debugging\n        import traceback\n        print(f\"Error processing job {job_name}:\")\n        print(traceback.format_exc())\n        \n        # Clean up on error\n        if job_dir.exists():\n            shutil.rmtree(job_dir, ignore_errors=True)\n        \n        # Return detailed error for debugging\n        raise HTTPException(\n            status_code=500, \n            detail=f\"Processing failed: {str(e)}\"\n        )\n    \n    finally:\n        # Clean up temporary files (keep dataset for debugging if needed)\n        if job_dir.exists():\n            shutil.rmtree(job_dir, ignore_errors=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Launch Server with ngrok"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Start the server\nimport threading\n\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n# Start server thread\nserver_thread = threading.Thread(target=run_server, daemon=True)\nserver_thread.start()\n\n# Wait for server to start\nimport time\ntime.sleep(3)\n\n# Create ngrok tunnel\ntunnel = ngrok.connect(8000)\npublic_url = tunnel.public_url\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"ðŸš€ 3D Gaussian Splatting API is live at: {public_url}\")\nprint(\"=\"*60)\nprint(f\"\\nTest with:\")\nprint(f\"export PUBLIC_URL='{public_url}'\")\nprint(f\"export API_KEY='{API_KEY}'\")\nprint(f\"\\nFor multiple images:\")\nprint(f'curl -X POST $PUBLIC_URL/process -H \"Api-Key: $API_KEY\" -F \"files=@img1.jpg\" -F \"files=@img2.jpg\" -F \"files=@img3.jpg\"')\nprint(f\"\\nFor video:\")\nprint(f'curl -X POST $PUBLIC_URL/process -H \"Api-Key: $API_KEY\" -F \"files=@video.mp4\" -F \"extract_fps=2\"')\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep alive\n",
    "print(\"\\nâ° Server is running. Keep this cell executing to maintain the connection.\")\n",
    "print(\"âš ï¸  Processing may take 10-30 minutes depending on image count and settings.\")\n",
    "print(\"Press 'Stop' to shutdown the server.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Server alive at: {public_url}\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nShutting down server...\")\n",
    "    ngrok.disconnect(public_url)\n",
    "    ngrok.kill()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}