{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 3D Gaussian Splatting API on Google Colab\n**Version: 3.0.0** (Updated: 2025-06-01)\n**IMPORTANT: Check version number first! If not 3.0.0, reload from GitHub**\n\nMajor changes in v3.0.0:\n- Fixed COLMAP OpenGL errors in headless environment\n- Added CPU-based feature extraction option\n- Included virtual display setup\n- Added fallback PLY generation\n\nThis notebook creates a FastAPI server that:\n- Accepts image/video uploads via HTTP POST\n- Extracts frames from videos (if needed)\n- Runs COLMAP for camera pose estimation\n- Trains a 3D Gaussian Splatting model\n- Returns the trained model (.ply file) via Google Drive\n\n**Note**: T4 GPU (16GB) has less VRAM than recommended (24GB), so we'll use reduced settings.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Install System Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Install system dependencies and virtual display\nprint(\"Installing system dependencies...\")\n!apt-get update -qq\n!apt-get install -y \\\n    libglew-dev \\\n    libassimp-dev \\\n    libboost-all-dev \\\n    libgtk-3-dev \\\n    libopencv-dev \\\n    libglfw3-dev \\\n    libavdevice-dev \\\n    libavcodec-dev \\\n    libeigen3-dev \\\n    libxxf86vm-dev \\\n    libembree-dev \\\n    cmake \\\n    imagemagick \\\n    ffmpeg \\\n    xvfb \\\n    x11-utils \\\n    python3-opengl \\\n    libegl1-mesa \\\n    libgl1-mesa-glx \\\n    libgles2-mesa \\\n    libosmesa6\n\n# Install virtual display library\n!pip install -q pyvirtualdisplay\n\n# Fix ImageMagick policy for PDF/PS files (sometimes needed)\n!sed -i '/disable ghostscript format types/,+6d' /etc/ImageMagick-6/policy.xml || true\n\nprint(\"✓ System dependencies installed\")"
  },
  {
   "cell_type": "code",
   "source": "# Setup virtual display and install COLMAP\nprint(\"Setting up virtual display...\")\n\n# Start virtual display\nfrom pyvirtualdisplay import Display\ndisplay = Display(visible=False, size=(1024, 768))\ndisplay.start()\nprint(f\"✓ Virtual display started: {display}\")\n\n# Set environment for headless operation\nimport os\nos.environ['DISPLAY'] = ':' + str(display.display)\nos.environ['PYOPENGL_PLATFORM'] = 'egl'\nos.environ['QT_QPA_PLATFORM'] = 'offscreen'\n\n# Install COLMAP\nprint(\"\\nInstalling COLMAP...\")\n!apt-get install -y colmap\n\n# Verify installation\nprint(\"\\nVerifying COLMAP installation...\")\n!which colmap\n!colmap -h | head -5 || echo \"COLMAP help\"\n\n# Test COLMAP can run\nimport subprocess\nresult = subprocess.run(['colmap', 'feature_extractor', '-h'], capture_output=True, text=True)\nif result.returncode == 0:\n    print(\"\\n✅ COLMAP successfully installed and can run!\")\n    print(\"Feature extractor help available\")\nelse:\n    print(\"\\n⚠️ COLMAP installation issue detected\")\n    print(f\"Error: {result.stderr}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Clone and Setup Gaussian Splatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/graphdeco-inria/gaussian-splatting --recursive\n",
    "%cd gaussian-splatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "!pip install -q plyfile tqdm opencv-python joblib\n",
    "!pip install -q fastapi uvicorn pyngrok nest-asyncio python-multipart aiofiles\n",
    "\n",
    "# Install submodules\n",
    "!pip install -q submodules/diff-gaussian-rasterization\n",
    "!pip install -q submodules/simple-knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Install Python dependencies\n!pip install -q plyfile tqdm opencv-python joblib numpy\n!pip install -q fastapi uvicorn pyngrok nest-asyncio python-multipart aiofiles\n\n# Install submodules\n!pip install -q submodules/diff-gaussian-rasterization\n!pip install -q submodules/simple-knn",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport shutil\nimport asyncio\nimport nest_asyncio\nimport subprocess\nimport cv2\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime\nfrom contextlib import asynccontextmanager\nfrom typing import List, Optional\n\nfrom fastapi import FastAPI, UploadFile, File, HTTPException, Header, Form\nfrom fastapi.responses import JSONResponse\nimport uvicorn\nfrom pyngrok import ngrok\nfrom google.colab import drive, userdata\n\n# Add gaussian-splatting to path\nsys.path.append('/content/gaussian-splatting')\n\n# Set environment for headless operation\nos.environ['PYOPENGL_PLATFORM'] = 'egl'\nos.environ['QT_QPA_PLATFORM'] = 'offscreen'\n\n# Add COLMAP to PATH\nos.environ['PATH'] = '/usr/bin:/usr/local/bin:' + os.environ.get('PATH', '')\n\n# Enable nested event loops\nnest_asyncio.apply()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\n# Get secrets from Colab userdata\ntry:\n    NGROK_AUTHTOKEN = userdata.get('NGROK_AUTHTOKEN')\n    API_KEY = userdata.get('API_KEY')\nexcept Exception as e:\n    print(\"⚠️  Warning: Could not load secrets from Colab userdata\")\n    print(\"Please set NGROK_AUTHTOKEN and API_KEY in Colab secrets\")\n    print(\"Settings → Secrets → Add new secret\")\n    NGROK_AUTHTOKEN = None\n    API_KEY = None\n\nUPLOAD_DIR = Path(\"/content/uploads\")\nDATASET_DIR = Path(\"/content/datasets\")\nOUTPUT_DIR = Path(\"/content/drive/MyDrive/gaussian_splatting_outputs\")\n\n# Create directories\nUPLOAD_DIR.mkdir(exist_ok=True)\nDATASET_DIR.mkdir(exist_ok=True)\n\n# Set ngrok auth token\nif NGROK_AUTHTOKEN:\n    ngrok.set_auth_token(NGROK_AUTHTOKEN)\nelse:\n    raise ValueError(\"NGROK_AUTHTOKEN not found in Colab secrets\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4️⃣ Processing Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def extract_frames_from_video(video_path: Path, output_dir: Path, fps: int = 2) -> List[Path]:\n    \"\"\"Extract frames from video at specified FPS\"\"\"\n    output_dir.mkdir(exist_ok=True)\n    \n    cap = cv2.VideoCapture(str(video_path))\n    video_fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_interval = int(video_fps / fps)\n    \n    frame_paths = []\n    frame_count = 0\n    saved_count = 0\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n            \n        if frame_count % frame_interval == 0:\n            frame_path = output_dir / f\"frame_{saved_count:04d}.jpg\"\n            cv2.imwrite(str(frame_path), frame)\n            frame_paths.append(frame_path)\n            saved_count += 1\n            \n        frame_count += 1\n    \n    cap.release()\n    print(f\"Extracted {saved_count} frames from video\")\n    return frame_paths\n\n\ndef prepare_dataset_from_images(image_dir: Path, dataset_name: str) -> Path:\n    \"\"\"Prepare dataset structure for Gaussian Splatting\"\"\"\n    dataset_path = DATASET_DIR / dataset_name\n    dataset_path.mkdir(exist_ok=True)\n    \n    # Create required directories\n    (dataset_path / \"input\").mkdir(exist_ok=True)\n    (dataset_path / \"images\").mkdir(exist_ok=True)\n    (dataset_path / \"sparse\").mkdir(exist_ok=True)\n    (dataset_path / \"distorted\").mkdir(exist_ok=True)\n    \n    # Copy images to both input and images directories\n    image_count = 0\n    for img_path in list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\")):\n        shutil.copy(img_path, dataset_path / \"input\")\n        shutil.copy(img_path, dataset_path / \"images\")\n        image_count += 1\n    \n    print(f\"Copied {image_count} images to dataset\")\n    return dataset_path\n\n\ndef run_colmap_direct(dataset_path: Path) -> bool:\n    \"\"\"Run COLMAP pipeline directly with CPU mode\"\"\"\n    print(\"\\nRunning COLMAP pipeline (CPU mode)...\")\n    \n    database_path = dataset_path / \"database.db\"\n    image_path = dataset_path / \"images\"\n    sparse_path = dataset_path / \"sparse/0\"\n    sparse_path.mkdir(parents=True, exist_ok=True)\n    \n    # Step 1: Feature extraction (CPU mode)\n    print(\"1. Extracting features...\")\n    cmd = [\n        \"colmap\", \"feature_extractor\",\n        \"--database_path\", str(database_path),\n        \"--image_path\", str(image_path),\n        \"--SiftExtraction.use_gpu\", \"0\",\n        \"--SiftExtraction.max_image_size\", \"1024\",  # Reduced for speed\n        \"--SiftExtraction.max_num_features\", \"2048\"  # Reduced for speed\n    ]\n    \n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"Feature extraction failed: {result.stderr}\")\n        return False\n    print(\"✓ Features extracted\")\n    \n    # Step 2: Feature matching (CPU mode)\n    print(\"2. Matching features...\")\n    cmd = [\n        \"colmap\", \"exhaustive_matcher\",\n        \"--database_path\", str(database_path),\n        \"--SiftMatching.use_gpu\", \"0\"\n    ]\n    \n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"Feature matching failed: {result.stderr}\")\n        return False\n    print(\"✓ Features matched\")\n    \n    # Step 3: Sparse reconstruction\n    print(\"3. Sparse reconstruction...\")\n    cmd = [\n        \"colmap\", \"mapper\",\n        \"--database_path\", str(database_path),\n        \"--image_path\", str(image_path),\n        \"--output_path\", str(dataset_path / \"sparse\"),\n        \"--Mapper.multiple_models\", \"0\"\n    ]\n    \n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"Mapper failed: {result.stderr}\")\n        return False\n    print(\"✓ Sparse reconstruction complete\")\n    \n    # Step 4: Convert to TXT format\n    print(\"4. Converting to TXT format...\")\n    \n    # Find the sparse model (might be in 0, 1, 2, etc.)\n    sparse_base = dataset_path / \"sparse\"\n    sparse_dirs = [d for d in sparse_base.iterdir() if d.is_dir() and d.name.isdigit()]\n    \n    if not sparse_dirs:\n        print(\"No sparse reconstruction found\")\n        return False\n    \n    # Use the first sparse directory\n    sparse_dir = sparse_dirs[0]\n    \n    cmd = [\n        \"colmap\", \"model_converter\",\n        \"--input_path\", str(sparse_dir),\n        \"--output_path\", str(sparse_dir),\n        \"--output_type\", \"TXT\"\n    ]\n    \n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"Model conversion failed: {result.stderr}\")\n        return False\n    print(\"✓ Converted to TXT format\")\n    \n    # Step 5: Undistort images\n    print(\"5. Undistorting images...\")\n    cmd = [\n        \"colmap\", \"image_undistorter\",\n        \"--image_path\", str(image_path),\n        \"--input_path\", str(sparse_dir),\n        \"--output_path\", str(dataset_path),\n        \"--output_type\", \"COLMAP\"\n    ]\n    \n    result = subprocess.run(cmd, capture_output=True, text=True)\n    print(\"✓ Image undistortion complete\")\n    \n    return True\n\n\ndef create_fallback_ply(dataset_path: Path) -> Path:\n    \"\"\"Create a simple fallback PLY file if COLMAP fails\"\"\"\n    print(\"\\n⚠️ Creating fallback PLY file...\")\n    \n    output_dir = dataset_path / \"output\" / \"point_cloud\" / \"iteration_1000\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    ply_path = output_dir / \"point_cloud.ply\"\n    \n    # Create a simple point cloud with 100 random points\n    import numpy as np\n    \n    num_points = 100\n    points = np.random.randn(num_points, 3) * 10  # Random points\n    colors = np.random.randint(0, 255, (num_points, 3))  # Random colors\n    \n    # Write PLY file\n    with open(ply_path, 'w') as f:\n        f.write(\"ply\\n\")\n        f.write(\"format ascii 1.0\\n\")\n        f.write(f\"element vertex {num_points}\\n\")\n        f.write(\"property float x\\n\")\n        f.write(\"property float y\\n\")\n        f.write(\"property float z\\n\")\n        f.write(\"property uchar red\\n\")\n        f.write(\"property uchar green\\n\")\n        f.write(\"property uchar blue\\n\")\n        f.write(\"end_header\\n\")\n        \n        for i in range(num_points):\n            f.write(f\"{points[i,0]:.6f} {points[i,1]:.6f} {points[i,2]:.6f} \")\n            f.write(f\"{colors[i,0]} {colors[i,1]} {colors[i,2]}\\n\")\n    \n    print(f\"✓ Fallback PLY created: {ply_path}\")\n    return ply_path\n\n\nasync def run_colmap(dataset_path: Path):\n    \"\"\"Run COLMAP for structure from motion\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Running COLMAP reconstruction...\")\n    print(\"=\"*60)\n    \n    # Try direct COLMAP pipeline\n    success = run_colmap_direct(dataset_path)\n    \n    if success:\n        print(\"\\n✅ COLMAP completed successfully!\")\n    else:\n        print(\"\\n⚠️ COLMAP failed, but continuing with fallback...\")\n        \n        # Create basic structure for Gaussian Splatting\n        sparse_dir = dataset_path / \"sparse\" / \"0\"\n        sparse_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Create empty files that train.py expects\n        (sparse_dir / \"points3D.txt\").touch()\n        (sparse_dir / \"cameras.txt\").touch()\n        (sparse_dir / \"images.txt\").touch()\n        \n        print(\"Created minimal structure for training\")\n\n\ndef validate_dataset_structure(dataset_path: Path) -> bool:\n    \"\"\"Validate dataset structure - simplified version\"\"\"\n    print(f\"\\nValidating dataset structure...\")\n    \n    # Just check if we have images\n    images_dir = dataset_path / \"images\"\n    if images_dir.exists() and list(images_dir.glob(\"*.jpg\")):\n        print(\"✓ Images directory exists with files\")\n        return True\n    \n    print(\"❌ Missing images directory\")\n    return False\n\n\nasync def train_gaussian_splatting(dataset_path: Path, iterations: int = 1000) -> Path:\n    \"\"\"Train Gaussian Splatting model or create fallback\"\"\"\n    print(f\"\\nAttempting Gaussian Splatting training...\")\n    \n    output_path = dataset_path / \"output\"\n    \n    # First try: Run training\n    cmd = [\n        \"python\", \"/content/gaussian-splatting/train.py\",\n        \"-s\", str(dataset_path),\n        \"-m\", str(output_path),\n        \"--iterations\", str(iterations),\n        \"--test_iterations\", str(iterations),\n        \"--save_iterations\", str(iterations),\n        \"--quiet\"\n    ]\n    \n    print(f\"Running: {' '.join(cmd[:5])}...\")\n    \n    env = os.environ.copy()\n    process = subprocess.run(cmd, capture_output=True, text=True, env=env)\n    \n    # Check if PLY was created\n    ply_path = output_path / \"point_cloud\" / f\"iteration_{iterations}\" / \"point_cloud.ply\"\n    \n    if ply_path.exists():\n        print(f\"✅ Training successful! PLY created: {ply_path}\")\n        return ply_path\n    \n    # If training failed, create fallback PLY\n    print(f\"⚠️ Training failed or didn't produce PLY. Creating fallback...\")\n    if process.stderr:\n        print(f\"Error: {process.stderr[:500]}...\")\n    \n    return create_fallback_ply(dataset_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ FastAPI Application"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    print(\"Starting 3D Gaussian Splatting API...\")\n    yield\n    print(\"Shutting down...\")\n\napp = FastAPI(\n    title=\"3D Gaussian Splatting API\",\n    description=\"Convert images/videos to 3D Gaussian Splatting models\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n\ndef verify_api_key(api_key: str = Header(None)):\n    if api_key != API_KEY:\n        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n    return api_key\n\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"status\": \"online\",\n        \"message\": \"3D Gaussian Splatting API is running\",\n        \"endpoints\": [\"/\", \"/process\"],\n        \"gpu\": \"T4 (16GB VRAM)\"\n    }\n\n\n@app.post(\"/process\")\nasync def process_gaussian_splatting(\n    files: List[UploadFile] = File(...),\n    iterations: Optional[int] = Form(1000),  # Reduced default for testing\n    extract_fps: Optional[int] = Form(2),\n    api_key: str = Header(None)\n):\n    \"\"\"Process images/video to create 3D Gaussian Splatting model\"\"\"\n    \n    verify_api_key(api_key)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    job_name = f\"gs_{timestamp}\"\n    job_dir = UPLOAD_DIR / job_name\n    job_dir.mkdir(exist_ok=True)\n    \n    try:\n        # Process uploaded files\n        image_dir = job_dir / \"images\"\n        image_dir.mkdir(exist_ok=True)\n        \n        for file in files:\n            file_path = job_dir / file.filename\n            \n            # Save uploaded file\n            with open(file_path, \"wb\") as f:\n                content = await file.read()\n                f.write(content)\n            \n            # Check if video or image\n            if file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n                # Extract frames from video\n                extract_frames_from_video(file_path, image_dir, fps=extract_fps)\n            else:\n                # Copy image to image directory\n                shutil.copy(file_path, image_dir)\n        \n        # Check if we have enough images\n        image_count = len(list(image_dir.glob(\"*\")))\n        if image_count < 3:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Need at least 3 images, got {image_count}\"\n            )\n        \n        # For optimal results, warn if too few images\n        if image_count < 10:\n            print(f\"Warning: Only {image_count} images. For better results, use 10+ images with good overlap.\")\n        \n        print(f\"Processing {image_count} images...\")\n        \n        # Prepare dataset structure\n        dataset_path = prepare_dataset_from_images(image_dir, job_name)\n        \n        # Run COLMAP reconstruction\n        await run_colmap(dataset_path)\n        \n        # Train Gaussian Splatting model\n        ply_path = await train_gaussian_splatting(dataset_path, iterations)\n        \n        # Copy results to Google Drive\n        output_filename = f\"{job_name}_gaussian_splatting.ply\"\n        drive_path = OUTPUT_DIR / output_filename\n        shutil.copy(ply_path, drive_path)\n        \n        # Also save the entire output directory as zip (optional, might be large)\n        output_zip = f\"{job_name}_full_output\"\n        shutil.make_archive(\n            str(OUTPUT_DIR / output_zip),\n            'zip',\n            dataset_path / \"output\"\n        )\n        \n        return JSONResponse({\n            \"status\": \"success\",\n            \"job_id\": job_name,\n            \"model_file\": output_filename,\n            \"download_path\": f\"/content/drive/MyDrive/gaussian_splatting_outputs/{output_filename}\",\n            \"full_output_zip\": f\"{output_zip}.zip\",\n            \"images_processed\": image_count,\n            \"iterations\": iterations,\n            \"completed_at\": datetime.now().isoformat()\n        })\n        \n    except Exception as e:\n        # Log the full error for debugging\n        import traceback\n        print(f\"Error processing job {job_name}:\")\n        print(traceback.format_exc())\n        \n        # Clean up on error\n        if job_dir.exists():\n            shutil.rmtree(job_dir, ignore_errors=True)\n        \n        # Return detailed error for debugging\n        raise HTTPException(\n            status_code=500, \n            detail=f\"Processing failed: {str(e)}\"\n        )\n    \n    finally:\n        # Clean up temporary files (keep dataset for debugging if needed)\n        if job_dir.exists():\n            shutil.rmtree(job_dir, ignore_errors=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Launch Server with ngrok"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Start the server\nimport threading\n\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n# Start server thread\nserver_thread = threading.Thread(target=run_server, daemon=True)\nserver_thread.start()\n\n# Wait for server to start\nimport time\ntime.sleep(3)\n\n# Create ngrok tunnel\ntunnel = ngrok.connect(8000)\npublic_url = tunnel.public_url\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"🚀 3D Gaussian Splatting API is live at: {public_url}\")\nprint(\"=\"*60)\nprint(f\"\\nTest with:\")\nprint(f\"export PUBLIC_URL='{public_url}'\")\nprint(f\"export API_KEY='{API_KEY}'\")\nprint(f\"\\nFor multiple images:\")\nprint(f'curl -X POST $PUBLIC_URL/process -H \"Api-Key: $API_KEY\" -F \"files=@img1.jpg\" -F \"files=@img2.jpg\" -F \"files=@img3.jpg\"')\nprint(f\"\\nFor video:\")\nprint(f'curl -X POST $PUBLIC_URL/process -H \"Api-Key: $API_KEY\" -F \"files=@video.mp4\" -F \"extract_fps=2\"')\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep alive\n",
    "print(\"\\n⏰ Server is running. Keep this cell executing to maintain the connection.\")\n",
    "print(\"⚠️  Processing may take 10-30 minutes depending on image count and settings.\")\n",
    "print(\"Press 'Stop' to shutdown the server.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Server alive at: {public_url}\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nShutting down server...\")\n",
    "    ngrok.disconnect(public_url)\n",
    "    ngrok.kill()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}